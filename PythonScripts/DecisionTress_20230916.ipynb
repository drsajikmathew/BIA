{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees\n",
    "\n",
    "The purpose of this chapter is to use the python libraries given by scikit learn to solve the case of Adventure Works targeting problem.\n",
    "DATA SET: Bikebuyer.csv\n",
    "The company's data warehouse, Adventure Works DW, contains a list of past customers with their demographic data (TargetMail) with attributes such as ‘Bikebuyer’, MaritalStatus', 'YearlyIncome', 'TotalChildren', 'ChildrenAtHome', 'HouseOwnerFlag','NumberCarsOwned','Age'\n",
    "\n",
    "CLASS WORK\n",
    "• Develop a decision tree using CART algorithm with Python libraries. Determine suitable ccp alpha for tree pruning.\n",
    "• Browse the tree to discover the classification of the records based on probability of buying cycles.\n",
    "• Analyse the prediction performance of the tree model using confusion matrix.\n",
    "• Build and test a DT using Random forests.\n",
    "• Compare performances of the two trees using ROC and PR curves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install updates (-U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT PYTHON MODULES\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import export_graphviz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification #for bootstrapping\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ AND EXPLORE DATA\n",
    "#Use pandas read_csv function to upload Bikebuyer data from the computer folder (Mac) to IPython. For other OS, please refer to Chapter 6 of Wes McKinney’s book on Python for Data Analysis\n",
    "#Alternatively upload data file to jupyter homepage and use pd.read_csv(\"Bikebuyer.csv\") \n",
    "data=pd.read_csv(\"/Users/sajimathew/Documents/DoMS/Teaching/EMBA/DMBI/DMBI-2020/Data/Bikebuyer.csv\")\n",
    "#Extract relevant features from data\n",
    "X=data[['YearlyIncome','TotalChildren','ChildrenAtHome','HouseOwnerFlag','NumberCarsOwned','Age']]\n",
    "#'MaritalStatus' not included as scikit learn CART algorithm doesnt support categorical variables type(X)\n",
    "#np.shape(X) # dimension of dataframe\n",
    "#X.dtypes\n",
    "#X.head()\n",
    "#data.isnull().sum().sum()\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PREPARATION\n",
    "#scikit-learn uses an optimised version of the CART algorithm; however, scikit-learn implementation does not support categorical variables for now.\n",
    "#[https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart]\n",
    "\n",
    "#Assign Bikebuyer as target variable y\n",
    "y=data['BikeBuyer']\n",
    "#Normalising predictor variables-not recommended for this algorithm, as integer data is preferred.\n",
    "#scaler = StandardScaler()\n",
    "#scaler.fit(X)\n",
    "#X=scaler.transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL BUILDING\n",
    "#Specify decision tree model as dt using scikit learn “DecisionTreeClassifier” module: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "dt=DecisionTreeClassifier(criterion='gini',\n",
    "    min_samples_split=1200,\n",
    "    min_samples_leaf=500,\n",
    "    ccp_alpha=0.0)\n",
    "#Fit the model using fit() method\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TREE PRUNING USING CCP ALPHA\n",
    "#Determining Cost Complexity Parameter (ccp_alpha) for post pruning the tree: https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py\n",
    "path = dt.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "dts = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    dt = DecisionTreeClassifier(criterion = \"gini\",  random_state=0, ccp_alpha=ccp_alpha)\n",
    "    dt.fit(X_train, y_train)\n",
    "    dts.append(dt)\n",
    "\n",
    "print(\n",
    "    \"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n",
    "        dts[-1].tree_.node_count, ccp_alphas[-1]\n",
    "    )\n",
    ")\n",
    "#checking if number of nodes becomes 1 for the last tree, if so remove it using the steps below\n",
    "#--removes last item if required\n",
    "#dts = dts[:-1]\n",
    "#ccp_alphas = ccp_alphas[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decide on max depth parameter\n",
    "node_counts = [dt.tree_.node_count for dt in dts]\n",
    "depth = [dt.tree_.max_depth for dt in dts]\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "ax[0].plot(ccp_alphas, node_counts, marker=\"o\", drawstyle=\"steps-post\")\n",
    "ax[0].set_xlabel(\"alpha\")\n",
    "ax[0].set_ylabel(\"number of nodes\")\n",
    "ax[0].set_title(\"Number of nodes vs alpha\")\n",
    "ax[1].plot(ccp_alphas, depth, marker=\"o\", drawstyle=\"steps-post\")\n",
    "ax[1].set_xlabel(\"alpha\")\n",
    "ax[1].set_ylabel(\"depth of tree\")\n",
    "ax[1].set_title(\"Depth vs alpha\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decide on ccp alpha baased on training and test error\n",
    "train_scores = [dt.score(X_train, y_train) for dt in dts]\n",
    "test_scores = [dt.score(X_test, y_test) for dt in dts]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
    "ax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, test_scores, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINAL MODEL BUILDING\n",
    "dt_cart=DecisionTreeClassifier(criterion='gini',max_depth= 10, \n",
    "    min_samples_split=1200,\n",
    "    min_samples_leaf=500,\n",
    "    ccp_alpha=0.0)\n",
    "dt_cart.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZING THE TREE USING GRAPHVIZ\n",
    "dot_data = tree.export_graphviz(dt_cart, out_file=None,feature_names=X_train.columns,class_names=['NonBuyer','Buyer'],filled=True, rounded=True,special_characters=True) \n",
    "graph = graphviz.Source(dot_data)\n",
    "graph\n",
    "#Scikit learn CART algorithm treats ordinal data also as integers and performs division. Categorical data not advisable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL PREDICTION AND PERFORMANCE\n",
    "y_pred=dt_cart.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "tn, fp, fn, tp\n",
    "#print(classification_report(y_test, y_pred, target_names=['Buyer','Nonbuyer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANDOM FOREST\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=4,\n",
    "                           n_informative=2, n_redundant=0,\n",
    "                           random_state=0, shuffle=False)\n",
    "#Generates 1000 subsamples for tree\n",
    "dt_rf=RandomForestClassifier(n_estimators=5, criterion='gini', \n",
    "                          max_depth=None, min_samples_split=1200, \n",
    "                          min_samples_leaf=500, \t\n",
    "                          ccp_alpha=0.0)\n",
    "dt_rf.fit(X_train, y_train)\n",
    "y_pred=dt_rf.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "tn, fp, fn, tp\n",
    "#print(classification_report(y_test, y_pred, target_names=['Buyer','Nonbuyer'])) \n",
    "#The F1 score can be interpreted as a harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
    "\n",
    "#F1 = 2 * (precision * recall) / (precision + recall) [Harmonic mean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC Curve and PR curves\n",
    "#References\n",
    "# 1. https://www.kaggle.com/code/nicholasgah/obtain-optimal-probability-threshold-using-rocfrom sklearn import metrics\n",
    "# 2. https://sklearn-evaluation.ploomber.io/en/latest/classification/micro_macro.html#:~:text=%2C%20%22C%22%5D-,Micro%2Daverage%20approach,(FNs)%20of%20the%20model.\n",
    "# 3. https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics\n",
    "# 4. https://scikit-learn.org/stable/visualizations.html\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve\n",
    "roc_cart=metrics.plot_roc_curve(dt_cart, X_test, y_test)\n",
    "roc_rf=metrics.plot_roc_curve(dt_rf, X_test, y_test)\n",
    "#plt.plot([0,1], [0,1], linestyle=\"--\")\n",
    "pr_cart=metrics.plot_precision_recall_curve(dt_cart, X_test, y_test)\n",
    "pr_rf=metrics.plot_precision_recall_curve(dt_rf, X_test, y_test)\n",
    "fpr, tpr, cutoffs = roc_curve(y_test, dt_cart.predict_proba(X_test)[:, 1])\n",
    "cutoffs\n",
    "#As many threshols as the number of leaf nodes; each score is the probability of the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subplots visualization of ROC and PR charts\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "roc_cart.plot(ax=ax1)\n",
    "roc_rf.plot(ax=ax1)\n",
    "pr_cart.plot(ax=ax2)\n",
    "pr_rf.plot(ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CROSS VALIDATION\n",
    "\n",
    "#simple validation set approach: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter  \n",
    "#by default splits into five folds\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score\n",
    "scores = cross_validate(dt, X_test, y_test)\n",
    "k=scores['test_score']\n",
    "k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-fold\n",
    "\n",
    "kf = KFold(n_splits=500)\n",
    "scores=[]\n",
    "for train_index, test_index in kf.split(X,y):\n",
    "    X_train, X_test, y_train, y_test = X[train_index], X[test_index],y[train_index], y[test_index]\n",
    "    dt.fit(X_train, y_train)\n",
    "    scores.append(dt.score(X_test, y_test)) \n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
